import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.metrics import accuracy_score
from torch.utils.data import DataLoader, TensorDataset
import numpy as np
import pandas as pd

# è¶…å‚æ•°
EPOCHS = 20
BATCH_SIZE = 32
LEARNING_RATE = 0.001
HIDDEN_SIZE = 64
INPUT_SIZE = 24  # è¾“å…¥ç‰¹å¾æ•°ï¼ˆä¸ç‰¹å¾åˆ—å¯¹åº”ï¼‰
OUTPUT_SIZE = 4  # è¾“å‡ºç±»åˆ«æ•°ï¼ˆactionTypeç±»åˆ«æ•°ï¼‰

# åŠ è½½æ•°æ®é›†
def load_data(csv_path):
    """è¯»å– CSV æ–‡ä»¶ï¼Œè¿”å› DataFrame"""
    print(f"ğŸš€ æ­£åœ¨åŠ è½½æ•°æ®æ–‡ä»¶ï¼š{csv_path}")
    try:
        df = pd.read_csv(csv_path)
        print(f"âœ… æˆåŠŸè¯»å– CSV æ–‡ä»¶ï¼Œæ ·æœ¬æ•°é‡ï¼š{df.shape[0]}")
        return df
    except Exception as e:
        print(f"âŒ è¯»å– CSV æ–‡ä»¶ {csv_path} å‡ºé”™ï¼š{e}")
        return None

# æ•°æ®å‡†å¤‡
def prepare_data(df, feature_columns, label_column="actionType"):
    """
    æ ¹æ®æŒ‡å®šçš„ç‰¹å¾åˆ—å’Œæ ‡ç­¾åˆ—ï¼Œå°† DataFrame åˆ†ç¦»ä¸ºç‰¹å¾çŸ©é˜µ X å’Œæ ‡ç­¾å‘é‡ yã€‚
    """
    print("ğŸš€ æ­£åœ¨å‡†å¤‡æ•°æ®ï¼ˆåˆ†ç¦»ç‰¹å¾å’Œæ ‡ç­¾ï¼‰...")
    if label_column not in df.columns:
        print(f"âŒ æ ‡ç­¾åˆ— {label_column} ä¸å­˜åœ¨ï¼")
        return None, None
    try:
        X = df[feature_columns].values
        y = df[label_column].values
        print(f"âœ… ç‰¹å¾æ•°æ®ç»´åº¦ï¼š{X.shape}ï¼Œæ ‡ç­¾æ•°é‡ï¼š{len(y)}")
        # å°†æ ‡ç­¾è½¬åŒ–ä¸ºæ•´æ•°
        label_map = {label: idx for idx, label in enumerate(np.unique(y))}
        y = np.array([label_map[label] for label in y])
        y_tensor = torch.tensor(y, dtype=torch.long)  # åˆ†ç±»é—®é¢˜æ ‡ç­¾åº”ä¸º long ç±»å‹
        X_tensor = torch.tensor(X, dtype=torch.float32)
        return X_tensor, y_tensor
    except Exception as e:
        print(f"âŒ æ•°æ®åˆ†ç¦»å‡ºé”™ï¼š{e}")
        return None, None

# å®šä¹‰ç¥ç»ç½‘ç»œæ¨¡å‹
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)  # è¾“å…¥å±‚åˆ°éšè—å±‚
        self.fc2 = nn.Linear(hidden_size, output_size)  # éšè—å±‚åˆ°è¾“å‡ºå±‚
        self.relu = nn.ReLU()  # æ¿€æ´»å‡½æ•°
        self.dropout = nn.Dropout(0.4)  # Dropoutå±‚ï¼Œ50%çš„ä¸¢å¼ƒç‡

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.dropout(x)  # æ·»åŠ Dropoutå±‚
        x = self.fc2(x)
        return x

# è®­ç»ƒå‡½æ•°
def train_nn(model, train_loader, criterion, optimizer):
    model.train()
    running_loss = 0.0
    correct_preds = 0
    total_preds = 0
    
    for inputs, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        correct_preds += (predicted == labels).sum().item()
        total_preds += labels.size(0)
    
    accuracy = 100 * correct_preds / total_preds
    avg_loss = running_loss / len(train_loader)
    return avg_loss, accuracy

# è¯„ä¼°å‡½æ•°
def evaluate_model(model, val_loader):
    model.eval()
    correct_preds = 0
    total_preds = 0
    with torch.no_grad():
        for inputs, labels in val_loader:
            outputs = model(inputs)
            _, predicted = torch.max(outputs, 1)
            correct_preds += (predicted == labels).sum().item()
            total_preds += labels.size(0)
    
    accuracy = 100 * correct_preds / total_preds
    return accuracy

# æ—©åœæœºåˆ¶
def early_stopping(verify_acc, best_val_acc, patience, no_improvement):
    if verify_acc > best_val_acc:
        best_val_acc = verify_acc
        no_improvement = 0
    else:
        no_improvement += 1
        if no_improvement >= patience:
            print("ğŸš¨ éªŒè¯é›†å‡†ç¡®ç‡æ²¡æœ‰æå‡ï¼Œæå‰åœæ­¢è®­ç»ƒï¼")
            return True, best_val_acc, no_improvement
    return False, best_val_acc, no_improvement

# ä¸»å‡½æ•°
if __name__ == "__main__":
    # è®¾ç½®è·¯å¾„å’Œæ–‡ä»¶
    TRAIN_DATA_CSV = "D:/vscode_work/badminton_classification/data/processed/processed_train_fft_normalized.csv"
    VERIFY_DATA_CSV = "D:/vscode_work/badminton_classification/data/processed/processed_verify_fft_normalized.csv"
    TEST_DATA_CSV = "D:/vscode_work/badminton_classification/data/processed/processed_test_fft_normalized.csv"
    
    # ç‰¹å¾åˆ—
    feature_columns = [
        "Ax_fft_mean", "Ax_fft_std", "Ax_fft_max", "Ax_dom_bin",
        "Ay_fft_mean", "Ay_fft_std", "Ay_fft_max", "Ay_dom_bin",
        "Az_fft_mean", "Az_fft_std", "Az_fft_max", "Az_dom_bin",
        "angularSpeedX_fft_mean", "angularSpeedX_fft_std", "angularSpeedX_fft_max", "angularSpeedX_dom_bin",
        "angularSpeedY_fft_mean", "angularSpeedY_fft_std", "angularSpeedY_fft_max", "angularSpeedY_dom_bin",
        "angularSpeedZ_fft_mean", "angularSpeedZ_fft_std", "angularSpeedZ_fft_max", "angularSpeedZ_dom_bin"
    ]
    
    # åŠ è½½è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®
    print("ğŸš€ åŠ è½½è®­ç»ƒé›†æ•°æ®...")
    train_df = load_data(TRAIN_DATA_CSV)
    print("ğŸš€ åŠ è½½éªŒè¯é›†æ•°æ®...")
    verify_df = load_data(VERIFY_DATA_CSV)
    print("ğŸš€ åŠ è½½æµ‹è¯•é›†æ•°æ®...")
    test_df = load_data(TEST_DATA_CSV)

    if train_df is None or verify_df is None or test_df is None:
        exit(1)

    # å‡†å¤‡è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•æ•°æ®
    X_train, y_train = prepare_data(train_df, feature_columns)
    X_verify, y_verify = prepare_data(verify_df, feature_columns)
    X_test, y_test = prepare_data(test_df, feature_columns)

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_dataset = TensorDataset(X_train, y_train)
    verify_dataset = TensorDataset(X_verify, y_verify)
    test_dataset = TensorDataset(X_test, y_test)

    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
    verify_loader = DataLoader(verify_dataset, batch_size=BATCH_SIZE, shuffle=False)
    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

    # å®šä¹‰æ¨¡å‹ã€æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    model = SimpleNN(input_size=INPUT_SIZE, hidden_size=HIDDEN_SIZE, output_size=OUTPUT_SIZE)
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

    # è®­ç»ƒè¿‡ç¨‹
    best_val_acc = 0
    patience = 5  # æå‰åœæ­¢çš„è€å¿ƒåº¦
    no_improvement = 0
    
    for epoch in range(EPOCHS):
        print(f"ğŸš€ è®­ç»ƒç¬¬ {epoch + 1} è½®...")
        train_loss, train_acc = train_nn(model, train_loader, criterion, optimizer)
        verify_acc = evaluate_model(model, verify_loader)
        print(f"âœ… è®­ç»ƒæŸå¤±: {train_loss:.4f}, è®­ç»ƒå‡†ç¡®ç‡: {train_acc:.2f}%")
        print(f"âœ… éªŒè¯å‡†ç¡®ç‡: {verify_acc:.2f}%")
        
        # æå‰åœæ­¢åˆ¤æ–­
        stop, best_val_acc, no_improvement = early_stopping(verify_acc, best_val_acc, patience, no_improvement)
        if stop:
            break
    
    # æµ‹è¯•é›†è¯„ä¼°
    test_acc = evaluate_model(model, test_loader)
    print(f"âœ… æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.2f}%")
